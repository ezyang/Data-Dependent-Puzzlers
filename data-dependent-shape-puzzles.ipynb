{"metadata":{"bento_stylesheets":{"bento\/extensions\/flow\/main.css":true,"bento\/extensions\/kernel_selector\/main.css":true,"bento\/extensions\/kernel_ui\/main.css":true,"bento\/extensions\/new_kernel\/main.css":true,"bento\/extensions\/system_usage\/main.css":true,"bento\/extensions\/theme\/main.css":true},"kernelspec":{"name":"bento_kernel_pytorch","display_name":"pytorch","language":"python","metadata":{"kernel_name":"bento_kernel_pytorch","nightly_builds":true,"fbpkg_supported":true,"cinder_runtime":true,"is_prebuilt":true,"default_tagged_version":"2290","kernel_version":"2290"},"isCinder":true},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text\/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3"},"last_server_session_id":"4c27f940-86dc-4eb0-96b1-9508b5150579","last_kernel_id":"2a815fdc-9a6a-4b77-94fa-af6c16f06f3b","last_base_url":"https:\/\/devgpu005.nha1.facebook.com:8091\/","last_msg_id":"522d962d-e2f4adeae2a077e0b4fd204e_1078","outputWidgetContext":[]},"nbformat":4,"nbformat_minor":2,"cells":[{"cell_type":"markdown","metadata":{"originalKey":"b21a6673-0c05-487a-b638-8d21462ce2b5","showInput":true,"customInput":null,"language":"markdown","outputsInitialized":false},"source":["When working on compiling or exporting code that has data dependent sizes, it's common to run into GuardOnDataDependentSymNode errors. Unfortunately, if it's the first time you've needed to fix one of these problems, it can be quite difficult to get your bearing in a real world model. The purpose of these puzzlers is to give you some practice fixing GuardOnDataDependentSymNode errors in a controlled setting, so you can get some sense for what works and doesn't and build your mental model.\n","\n","I highly recommend reading [Dealing with GuardOnDataDependentSymNode errors](https:\/\/docs.google.com\/document\/d\/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs\/edit#heading=h.44gwi83jepaj) document first."]},{"cell_type":"code","metadata":{"originalKey":"7b8a6a27-89b1-487f-b2b5-8cd46ba39280","showInput":true,"customInput":null,"language":"python","executionStartTime":1715561335166,"executionStopTime":1715561335640,"serverExecutionDuration":383.22223490104,"collapsed":false,"requestMsgId":"7b8a6a27-89b1-487f-b2b5-8cd46ba39280","outputsInitialized":false,"customOutput":null},"source":["!pip install --pre torch -f https:\/\/download.pytorch.org\/whl\/nightly\/cpu\/torch_nightly.html -U"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"originalKey":"4fb9ef34-21b8-472e-8891-afaa78059796","showInput":true,"customInput":null,"language":"python","executionStartTime":1715115122144,"executionStopTime":1715115125718,"serverExecutionDuration":3450.5126089789,"collapsed":false,"requestMsgId":"4fb9ef34-21b8-472e-8891-afaa78059796","outputsInitialized":false},"source":["import torch\n","import torch._dynamo.config\n","from typing import List, Union, Any\n","from torch import Tensor\n","from torch.fx.experimental.symbolic_shapes import guard_size_oblivious\n","from dataclasses import dataclass\n","import random"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":false,"originalKey":"3d2e81fe-23b4-43d4-81c3-df1cfa30ad56","outputsInitialized":false,"language":"python","executionStartTime":1715115125725,"executionStopTime":1715115125843,"serverExecutionDuration":11.734284926206,"requestMsgId":"3d2e81fe-23b4-43d4-81c3-df1cfa30ad56","showInput":false},"source":["torch._dynamo.config.capture_scalar_outputs = True\n","torch._dynamo.config.capture_dynamic_output_shape_ops = True\n","\n","TEMPLATE = True\n","MSG = \"\ud83c\udf89 Correct! \ud83c\udf89\"\n","\n","\u0040dataclass\n","class U:\n","    # Describes an unbacked SymInt whose real value is val, for testing\n","    val: int\n","\n","UnpackedTy = Union[int, U]\n","PackedTy = Union[int, Tensor]\n","\n","def assert_eq(actual, expected):\n","    if isinstance(actual, Tensor) or isinstance(expected, Tensor):\n","        assert isinstance(actual, Tensor), actual\n","        assert isinstance(expected, Tensor), expected\n","        cond = torch.allclose(actual, expected)\n","    elif isinstance(actual, list) or isinstance(expected, list):\n","        assert isinstance(actual, list), actual\n","        assert isinstance(expected, list), expected\n","        for a, e in zip(actual, expected):\n","            assert_eq(a, e)\n","        cond = len(actual) == len(expected)\n","    else:\n","        assert type(actual) == type(expected), (actual, expected)\n","        cond = actual == expected\n","        assert isinstance(cond, bool), (actual, expected)\n","    if not cond:\n","        raise RuntimeError(f\"{actual} != {expected}\")\n","\n","def pack_size(s: UnpackedTy) -> PackedTy:\n","    if isinstance(s, U):\n","        return torch.tensor(s.val)\n","    else:\n","        return s\n","\n","def pack_sizes(shape: UnpackedTy) -> List[PackedTy]:\n","    return [pack_size(s) for s in shape]\n","\n","# NB: the unpack functions will be Dynamo traced, so they're not really inverses\n","def unpack_size(s: PackedTy) -> int:\n","    if isinstance(s, Tensor):\n","        r = s.item()\n","        torch._check_is_size(r)\n","        return r\n","    else:\n","        return s\n","\n","def unpack_sizes(ss: List[PackedTy]) -> List[int]:\n","    return [unpack_size(s) for s in ss]\n","\n","def unpack_tensor(ss: List[PackedTy]) -> Tensor:\n","    return torch.randn(unpack_sizes(ss))\n","\n","def force_guard(x: bool):\n","    if x:\n","        return torch.tensor(True)\n","    else:\n","        return torch.tensor(False)\n","\n","def run_test(fn):\n","    torch._dynamo.reset()\n","    fn()\n","    print(MSG)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"originalKey":"822a585b-6c96-4d4a-ad0b-e0723dc7d0f3","showInput":false,"customInput":null,"language":"markdown","outputsInitialized":false},"source":["## Basic symbolic reasoning\n","\n","Here are some basic warmups to test your understanding of [Dealing with GuardOnDataDependentSymNode errors](https:\/\/docs.google.com\/document\/d\/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs\/edit#heading=h.44gwi83jepaj), and familiarize you with how this notebook is setup.\n","\n","**[check]** Use torch._check to resolve the compile time error below."]},{"cell_type":"code","metadata":{"originalKey":"aa26ecd6-28a2-4941-bc9b-d7fd40814bc9","showInput":true,"customInput":null,"language":"python","executionStartTime":1715277419005,"executionStopTime":1715277421046,"serverExecutionDuration":64.66610217467,"collapsed":false,"requestMsgId":"aa26ecd6-28a2-4941-bc9b-d7fd40814bc9","outputsInitialized":false},"source":["\u0040torch.compile(backend=\"eager\", fullgraph=True)\n","def cf_check(x):\n","    u0, u1 = x.tolist()\n","    # Do not modify the code below here (imagine it's in framework code you can't edit)\n","    # NB: In future exercises, we'll use force_guard as a shorthand for this pattern.\n","    if u0 * 2 == u1 * 3:\n","        return torch.tensor(True)\n","    else:\n","        return torch.tensor(False)\n","\n","\u0040run_test\n","def test_check():\n","    assert cf_check(torch.tensor([12, 8])).item()"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"originalKey":"5cc395a8-1414-4421-ba73-d7f6e37ed254","showInput":false,"customInput":null,"language":"markdown","outputsInitialized":false},"source":["**[checkand]**  It is best not to use logical conjunction inside `torch._check` operators.  Modify the torch._check below so that compilation passes."]},{"cell_type":"code","metadata":{"originalKey":"6f6b94a7-5224-4c25-8ec9-5cefdefaebb9","showInput":true,"customInput":null,"language":"python","executionStartTime":1715115148358,"executionStopTime":1715115148940,"serverExecutionDuration":433.30848496407,"collapsed":false,"requestMsgId":"6f6b94a7-5224-4c25-8ec9-5cefdefaebb9","outputsInitialized":false},"source":["\u0040torch.compile(backend=\"eager\", fullgraph=True)\n","def cf_checkand(x):\n","    u0, u1 = x.tolist()\n","    torch._check((u0 \/\/ 3 == 0) and (u1 \/\/ 5 == 0))\n","    # Do not modify the code below\n","    return force_guard((u0 \/\/ 3 == 0) and (u1 \/\/ 5 == 0))\n","\n","\u0040run_test\n","def test_checkand():\n","    assert cf_checkand(torch.tensor([2, 4]))"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"originalKey":"e6319303-010e-4212-b8a9-0c41ffe29dd2","showInput":false,"customInput":null,"language":"markdown","outputsInitialized":false},"source":["**[checksize]** Marking a variable as size-like means we will assume it is `>= 2` inside size oblivious guards, but we will still permit it to have 0\/1 value at runtime."]},{"cell_type":"code","metadata":{"originalKey":"75a4e0eb-6f36-4bbc-b2c0-750ce44e7a2d","showInput":true,"customInput":null,"language":"python","executionStartTime":1715359287976,"executionStopTime":1715359288142,"serverExecutionDuration":27.755219954997,"collapsed":false,"requestMsgId":"75a4e0eb-6f36-4bbc-b2c0-750ce44e7a2d","outputsInitialized":false},"source":["\u0040torch.compile(backend=\"eager\", fullgraph=True)\n","def cf_checksize(x):\n","    u0 = x.item()\n","    # For extra credit, mark u0 as size-like WITHOUT explicitly calling torch._check_is_size\n","    pass\n","    return force_guard(guard_size_oblivious(u0 != 0))\n","\n","\u0040run_test\n","def test_checksize():\n","    assert cf_checksize(torch.tensor(5)).item()\n","    assert cf_checksize(torch.tensor(0)).item()  # this is true!  You can make some strange things happen if you combine this with torch._check(u0 == 0)"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"originalKey":"e529114a-1119-4c36-8542-a7c5ea5fb5e7","showInput":false,"customInput":null,"language":"markdown","outputsInitialized":false},"source":["**[implicitsize]** Some APIs do not implicitly specify a size is size-like.  So you may need to explicitly specify it in those situations.  When this is not a bug in PyTorch, this is typically because the API accepts sentinel values like -1 and you have to explicitly indicate that this is not a permitted input."]},{"cell_type":"code","metadata":{"originalKey":"33aca77b-e13b-4ec2-aa47-fe3f551187ad","showInput":true,"customInput":null,"language":"python","executionStartTime":1715115157110,"executionStopTime":1715115159333,"serverExecutionDuration":2058.8403525762,"collapsed":false,"requestMsgId":"33aca77b-e13b-4ec2-aa47-fe3f551187ad","outputsInitialized":false},"source":["\u0040torch.compile(backend=\"eager\", fullgraph=True)\n","def cf_implicitsize(x, y):\n","    u0 = x.item()\n","    pass\n","    torch._check(u0 \u003C y.size(0))\n","    return y[u0]\n","\n","\u0040run_test\n","def test_implicitsize():\n","    assert cf_implicitsize(torch.tensor(2), torch.randn(10)).item()"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"originalKey":"4610cd31-bcf9-4fbd-bf65-a6c72f47fffa","showInput":false,"customInput":null,"language":"markdown","outputsInitialized":false},"source":["**[nomemo]** Some APIs in PyTorch accept Tensor directly where int is expected. This results in an implicit item() call. Although sometimes we memoize the returned unbacked SymInts, in general it is better to make sure you call item() once, and use the resulting unbacked SymInt consistently through the rest of the program.  Fix the program below.  Note that the commented `torch._check_is_size` does not work (why?)"]},{"cell_type":"code","metadata":{"originalKey":"2746e66a-ea0b-4853-88e5-fb30d9ecc722","showInput":true,"customInput":null,"language":"python","executionStartTime":1715115164322,"executionStopTime":1715115165894,"serverExecutionDuration":1397.3077549599,"collapsed":false,"requestMsgId":"2746e66a-ea0b-4853-88e5-fb30d9ecc722","outputsInitialized":false},"source":["\u0040torch.compile(fullgraph=True, backend=\"eager\")\n","def cf_nomemo(x, y):\n","    # torch._check_is_size(y[0])\n","    return x.unsqueeze(1).expand(-1, y[0])\n","\n","\u0040run_test\n","def test_nomemo():\n","    cf_nomemo(torch.randn(8), torch.tensor([2]))"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"originalKey":"325bac6a-49a0-4b6b-8f2e-d46ce57f7064","showInput":false,"customInput":null,"language":"markdown","outputsInitialized":false},"source":["## What's in a tensor constructor?\n","\n","The very first guards you are likely to encounter with data-dependent shapes are those associated with tensor constructors.  Modern PyTorch has been patched to work with data-dependent shapes; in this section, we reimplement various important functions that are called during tensor construction, to make them data-dependent shape friendly.  If you don't feel comfortable working with strides, consider reading the [PyTorch Internals](http:\/\/blog.ezyang.com\/2019\/05\/pytorch-internals\/) blog post.  We have already imported `guard_size_oblivious` so that it is in scope."]},{"cell_type":"markdown","metadata":{"originalKey":"621cbe5d-b4e0-4e87-ad20-8b4ac08b4898","showInput":false,"customInput":null,"language":"markdown","outputsInitialized":false},"source":["**[contigstride]**  When you call a constructor like torch.empty() which produces a contiguous tensor, we must compute the contiguous strides for the tensor. Intuitively, the stride of a contiguous tensor is the product of all sizes to the left of it; e.g., a tensor with size `[2, 3, 4]` has stride `[3*4, 4, 1]`.\n","\n","This calculation has a subtle special case for zero sized tensors (`t.numel() == 0`). Ordinarily, a stride tells us how much we must advance the physical pointer of a tensor to access the next element.  But in a zero element tensor, there is no next element: this means we could validly put whatever we want in the stride of a tensor.  The convention that Numpy and PyTorch have for this situation is, we compute the strides as normal, but if a size is zero, we treat it as if it had size one for the purpose of stride computation, preventing us from zeroing out any subsequent strides.\n","\n","Below, we've reproduced `make_contiguous_strides_for` function in PyTorch.  Modify it so that it works with data dependent sizes."]},{"cell_type":"code","metadata":{"originalKey":"5a16d659-6d2c-4edb-aa99-16160c57310e","showInput":true,"customInput":null,"language":"python","executionStartTime":1715359377329,"executionStopTime":1715359377595,"serverExecutionDuration":101.2082262896,"collapsed":false,"requestMsgId":"5a16d659-6d2c-4edb-aa99-16160c57310e","outputsInitialized":false,"customOutput":null},"source":["# Change this FUNCTION only\n","def make_contiguous_strides_for(shape: List[int]) -> List[int]:\n","    \"\"\"Returns the strides of a contiguous tensor.\"\"\"\n","    multiplier = 1 \n","    strides = []\n","    for l in reversed(shape):\n","        torch._check_is_size(l)\n","        strides.append(multiplier)\n","        if l >= 1:\n","            multiplier *= l\n","\n","    return list(reversed(strides))\n","\n","def f_contigstride(pxs):\n","    xs = unpack_sizes(pxs)\n","    return torch.empty_strided(xs, make_contiguous_strides_for(xs))\n","\n","cf_contigstride = torch.compile(fullgraph=True, backend=\"eager\")(f_contigstride)\n","\n","\u0040run_test\n","def test_contfor():\n","    inp1, ans1 = pack_sizes([U(2), U(3), U(4)]), (12, 4, 1)\n","    assert_eq(f_contigstride(inp1).stride(), ans1)\n","    assert_eq(cf_contigstride(inp1).stride(), ans1)\n","\n","    inp2, ans2 = pack_sizes([U(2), U(0), U(4)]), (4, 4, 1)\n","    assert_eq(f_contigstride(inp2).stride(), ans2)\n","    # Extra credit: can you make this test pass?\n","    # assert_eq(cf_contfor(inp2).stride(), ans2)"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"originalKey":"2c151833-13d3-45fb-b2db-166d8c072b62","showInput":false,"customInput":null,"language":"markdown","outputsInitialized":false},"source":["**[manualcontig]** Another thing we must do on tensor construction is compute if it is contiguous or not.  Although a call to torch.empty is obviously contiguous, constructors like torch.empty_strided allow construction of tensors with arbitrary strides, so we need an algorithm `is_contiguous` that can take the sizes and strides of a tensor and determine if it is contiguous.\n","\n","Modify the function below so that it works with data dependent sizes."]},{"cell_type":"code","metadata":{"originalKey":"bb0c28df-9b78-430b-8350-ee9a48bfd2c7","showInput":true,"customInput":null,"language":"python","executionStartTime":1715115199197,"executionStopTime":1715115200021,"serverExecutionDuration":576.66304893792,"collapsed":false,"requestMsgId":"bb0c28df-9b78-430b-8350-ee9a48bfd2c7","outputsInitialized":false},"source":["# Change this FUNCTION only\n","def is_contiguous(a: Tensor) -> bool:\n","    if a.numel() \u003C 2:\n","        return True\n","\n","    expected_stride = 1\n","    for x, y in reversed(tuple(zip(a.shape, a.stride()))):\n","        # Skips checking strides when a dimension has length 1\n","        if x == 1:\n","            continue\n","\n","        if y != expected_stride:\n","            return False\n","        expected_stride = expected_stride * x\n","\n","    return True   \n","\n","def f_manualcontig(x):\n","    return force_guard(is_contiguous(unpack_tensor(x)))\n","\n","cf_manualcontig = torch.compile(fullgraph=True, backend=\"eager\")(f_manualcontig)\n","\n","\u0040run_test\n","def test_manualcontig():\n","    inp1 = pack_sizes([U(2), U(3), U(4)])\n","    assert cf_manualcontig(inp1).item()"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"originalKey":"182f0562-c9b2-4c9f-8b73-57732262a312","showInput":false,"customInput":null,"language":"markdown","outputsInitialized":false},"source":["**[transposecontig]** In PyTorch, `is_contiguous` is implemented with `guard_size_oblivious`.  The use of guard_size_oblivous means that sometimes the semantics of plain and compiled programs can diverge.  The most obvious consequence of this is that with unbacked SymInts, we will generally conclude that tensors are noncontiguous, even if there are specific values for which they might actually be contiguous.  For example, we always consider a transposed 2D tensor of size `[u0, u1]` non-contiguous.\n","\n","In the test below, change `inp1` to show a 2D tensor which is contiguous in eager mode even after transposition.\n",""]},{"cell_type":"code","metadata":{"originalKey":"ee02b6f1-d075-49da-992f-b6c25727805e","showInput":true,"customInput":null,"language":"python","executionStartTime":1715174400998,"executionStopTime":1715174401407,"serverExecutionDuration":186.27102486789,"collapsed":false,"requestMsgId":"ee02b6f1-d075-49da-992f-b6c25727805e","outputsInitialized":false},"source":["def f_transposecontig(sizes):\n","    x = torch.zeros(sizes.tolist()).T\n","    return force_guard(x.is_contiguous())\n","\n","cf_transposecontig = torch.compile(fullgraph=True, backend=\"eager\")(f_transposecontig)\n","\n","\u0040run_test\n","def test_transposecontig():\n","    # Change the int values in pack_sizes ONLY\n","    inp1 = torch.tensor([3, 4])\n","\n","    assert not torch.allclose(f_transposecontig(inp1), cf_transposecontig(inp1)), f\"{f_transposecontig(inp1)} == {cf_transposecontig(inp1)}\""],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"originalKey":"bb92a62b-7f6d-425a-9d24-c11cc69d90d9","showInput":false,"customInput":null,"language":"markdown","outputsInitialized":false},"source":["A similar version of the problem above also occurs when answering `is_contiguous(memory_format=torch.channels_last)`, since NCHW contiguous (the default) tensors are typically not NHWC contiguous, except under the same conditions as the input above."]},{"cell_type":"markdown","metadata":{"originalKey":"fd29d1e9-33e3-4f44-991e-f564dac88cbf","showInput":false,"customInput":null,"language":"markdown","outputsInitialized":false},"source":["## Pointwise operations: broadcasting\n","\n","Whenever you perform a pointwise operation between two tensors, we must compute the output shape of the tensor. Because PyTorch supports implicit broadcasting, this is not simply \"assert the two input tensors have the same size and produce a tensor of the same size.\"  Instead, whenever you compare two dimensions for equality, you allow for a size mismatch if one of the dimensions is size one.  Unlike the previous examples, you will need to make more changes than just slapping `guard_size_oblivious` on the conditions."]},{"cell_type":"code","metadata":{"originalKey":"1beb85d6-106c-4698-a3fc-8e02f4c30b38","showInput":true,"customInput":null,"language":"python","outputsInitialized":false,"executionStartTime":1715115223375,"executionStopTime":1715115223686,"serverExecutionDuration":140.76845115051,"collapsed":false,"requestMsgId":"1beb85d6-106c-4698-a3fc-8e02f4c30b38"},"source":["def infer_size(a: List[int], b: List[int]) -> List[int]:\n","    dimsA = len(a)\n","    dimsB = len(b)\n","    ndim = max(dimsA, dimsB)\n","    expandedSizes = [0] * ndim\n","    for i in range(ndim - 1, -1, -1):\n","        offset = ndim - 1 - i\n","        dimA = dimsA - 1 - offset\n","        dimB = dimsB - 1 - offset\n","        sizeA = a[dimA] if dimA >= 0 else 1\n","        sizeB = b[dimB] if dimB >= 0 else 1\n","        torch._check_is_size(sizeA)\n","        torch._check_is_size(sizeB)\n","        if sizeA == sizeB:\n","            expandedSizes[i] = sizeA\n","        elif sizeA == 1:\n","            expandedSizes[i] = sizeB\n","        elif sizeB == 1:\n","            expandedSizes[i] = sizeA\n","        else:\n","            raise RuntimeError(\"shape mismatch\")\n","    return expandedSizes\n","\n","def f_infersize(sz1, sz2):\n","    return torch.zeros(infer_size(unpack_sizes(sz1), unpack_sizes(sz2)))\n","\n","cf_infersize = torch.compile(fullgraph=True, backend=\"eager\")(f_infersize)\n","\n","\u0040run_test\n","def test_infersize():\n","    # Some baseline tests: these should pass even without changes\n","    assert_eq(cf_infersize(pack_sizes([2, 3, 4]), pack_sizes([2, 3, 4])).shape, torch.Size([2, 3, 4]))\n","    assert_eq(cf_infersize(pack_sizes([2, 1, 4]), pack_sizes([2, 3, 4])).shape, torch.Size([2, 3, 4]))\n","    # Now test with unbacked SymInts\n","    assert_eq(cf_infersize(pack_sizes([2, 1, 4]), pack_sizes([2, U(3), 4])).shape, torch.Size([2, 3, 4]))\n","    # NB: The two occurrences of U(2) have distinct unbacked SymInts!\n","    assert_eq(cf_infersize(pack_sizes([U(2), 1, 4]), pack_sizes([U(2), U(3), 4])).shape, torch.Size([2, 3, 4]))"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"originalKey":"6b73f903-c554-4385-995f-7309ae225856","showInput":false,"customInput":null,"language":"markdown","outputsInitialized":false},"source":["## More advanced symbolic reasoning\n","\n","**[latespec]** Reasoning about guards is done *online* fashion; whenever we have to test a guard, we must immediately determine if it is true or false; otherwise tracing cannot continue.  This means that whether or not a program traces is order sensitive.\n","\n","Without removing the guard in question or adding any deferred runtime asserts to u0, modify this program so it compiles."]},{"cell_type":"code","metadata":{"originalKey":"95bd5fd6-439c-4082-a839-16642096a8b3","showInput":true,"customInput":null,"language":"python","executionStartTime":1715115231487,"executionStopTime":1715115232735,"serverExecutionDuration":1089.7224936634,"collapsed":false,"requestMsgId":"95bd5fd6-439c-4082-a839-16642096a8b3","outputsInitialized":false},"source":["\u0040torch.compile(dynamic=True, fullgraph=True, backend=\"eager\")\n","def cf_latespec(x, y):\n","    u0 = unpack_size(x)\n","    s0 = y.size(0)\n","    b = force_guard(u0 * s0 ** 2 == 9 * u0)  # do not change this line\n","    torch._check(s0 == 3)\n","    return b\n","\n","\u0040run_test\n","def test_latespec():\n","    assert cf_latespec(pack_size(U(10)), torch.randn(3)).item()"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"originalKey":"0a8504ff-cf2b-4b66-b685-94e8f3ecdb6d","showInput":false,"customInput":null,"language":"markdown","outputsInitialized":false},"source":["**[uselessunbacked]** Sometimes, poorly written user code reads out unbacked variables from Tensors for use with sizes, even when they actually statically *know* the size in question.  Rewriting code to preferentially used backed quantities, and only referencing unbacked code to generate runtime asserts, can help you bypass data dependent exceptions.\n","\n","Without changing the runtime semantics (including exception throwing behavior), modify this program so it compiles."]},{"cell_type":"code","metadata":{"originalKey":"af0c7e69-b845-490d-aff0-54a87dd93868","showInput":true,"customInput":null,"language":"python","executionStartTime":1715115239078,"executionStopTime":1715115240682,"serverExecutionDuration":1371.3203612715,"collapsed":false,"requestMsgId":"af0c7e69-b845-490d-aff0-54a87dd93868","outputsInitialized":false},"source":["# TODO: Inductor still seems to be dropping the asserts :think:\n","\u0040torch.compile(dynamic=True, fullgraph=True, backend=\"eager\")\n","def cf_uselessunbacked(x, y):\n","    u0 = unpack_size(x)\n","    x2 = torch.arange(u0 \/\/ 2)\n","    return x2 + y\n","\n","\u0040run_test\n","def test_uselessunbacked():\n","    assert_eq(cf_uselessunbacked(pack_size(U(20)), torch.zeros(10, dtype=torch.int64)), torch.arange(10,))\n","    try:\n","        cf_uselessunbacked(pack_size(U(10)), torch.zeros(10, dtype=torch.int64))\n","    except RuntimeError:\n","        pass\n","    else:\n","        raise AssertionError(\"Expected runtime error\")"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"originalKey":"b3bc494a-9c8c-4335-81a1-3cf0e9681545","showInput":false,"customInput":null,"language":"markdown","outputsInitialized":false},"source":["**[changevar]** When you call item() on a tensor, you trigger the allocation of a fresh unbacked variable.  It matters whether or not a given size is a variable `u0` or an expression `u0 \/\/ 2`, because we only ascribe value ranges to variables; expressions only ever have their value ranges derived from the value ranges of their free symbols.\n","\n","Without changing the valid range of values for x, modify the program below so that it compiles."]},{"cell_type":"code","metadata":{"originalKey":"9d6312a8-8315-453c-bcc7-8014d52d5b9a","showInput":true,"customInput":null,"language":"python","executionStartTime":1715115247204,"executionStopTime":1715115248446,"serverExecutionDuration":991.65102792904,"collapsed":false,"requestMsgId":"9d6312a8-8315-453c-bcc7-8014d52d5b9a","outputsInitialized":false},"source":["\u0040torch.compile(dynamic=True, fullgraph=True, backend=\"eager\")\n","def cf_changevar(x):\n","    u0 = x.item()\n","    torch._check_is_size(u0)\n","    r = torch.arange(u0 \/\/ 2)\n","    return r + r\n","\n","\u0040run_test\n","def test_changevar():\n","    assert_eq(cf_changevar(torch.tensor(20)), torch.arange(10) * 2)\n","    assert_eq(cf_changevar(torch.tensor(0)), torch.arange(0) * 2)"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"originalKey":"9e014a30-4151-46f3-8274-19bf0f071bed","showInput":false,"customInput":null,"language":"markdown","outputsInitialized":false},"source":["**[varrange]** Value ranges are a simple but powerful reasoning mechanism, whereby we propagate the lower and upper bounds of variables through expressions.  When you perform a `torch._check(u0 >= c)`, where c is a constant, you refine the value range of that variable.\n","\n","Using only one call to `torch._check` (do NOT use `torch._check_is_size`), discharge all of the guards in this program."]},{"cell_type":"code","metadata":{"originalKey":"816d8db7-5c3b-46a0-a67c-39e9568b0314","showInput":true,"customInput":null,"language":"python","executionStartTime":1715115253809,"executionStopTime":1715115255035,"serverExecutionDuration":974.59526918828,"collapsed":false,"requestMsgId":"816d8db7-5c3b-46a0-a67c-39e9568b0314","outputsInitialized":false},"source":["\u0040torch.compile(dynamic=True, fullgraph=True, backend=\"eager\")\n","def cf_varrange(x):\n","    u0 = x.item()\n","    # Put your torch._check call here\n","    pass\n","    a = force_guard(u0 != 0)\n","    b = force_guard(u0 != 1)\n","    c = force_guard(u0 != -1)\n","    d = force_guard(u0 * u0 != 0)\n","    e = force_guard(u0 >= 0)\n","    f = force_guard(u0 * u0 >= 2)\n","    return sum([a, b, c, d, e, f])\n","\n","\u0040run_test\n","def test_varrange():\n","    assert cf_varrange(torch.tensor(2)).item() == 6\n","    assert cf_varrange(torch.tensor(10)).item() == 6\n","    assert cf_varrange(torch.tensor(100)).item() == 6"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"originalKey":"1b0b60a0-6a17-4a20-b7b8-d5197b7aee4c","showInput":false,"customInput":null,"language":"markdown","outputsInitialized":false},"source":["## Judo moves\n","\n","Sometimes, fixing your code doesn't involve just bashing `torch._check` everywhere, you need a more semantic change, a judo move.\n","\n","**[stacklist]** Unbacked SymInts cannot be used to index into Python data structures.  However, if the data structure can be expressed as a single Tensor, it can be done symbolically"]},{"cell_type":"code","metadata":{"originalKey":"03d7e144-92e0-41a0-b905-0e211442b73d","showInput":true,"customInput":null,"language":"python","executionStartTime":1715115268351,"executionStopTime":1715115269360,"serverExecutionDuration":741.27985723317,"collapsed":false,"requestMsgId":"03d7e144-92e0-41a0-b905-0e211442b73d","outputsInitialized":false},"source":["\u0040torch.compile(fullgraph=True, backend=\"eager\")\n","def cf_stacklist(xs: List[Tensor], y: Tensor):\n","    u0 = y.item()\n","    torch._check_is_size(u0)\n","    torch._check(u0 \u003C len(xs))\n","    return xs[u0]\n","\n","\u0040run_test\n","def test_stacklist():\n","    assert_eq(cf_stacklist([torch.zeros(5), torch.ones(5)], torch.tensor(0)), torch.zeros(5))"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"originalKey":"8568fbb6-0bfe-4c3d-a9b3-4eead02713b8","showInput":false,"customInput":null,"language":"markdown","outputsInitialized":false},"source":["**[vb]** Some code in torchrec tests if all batches are the same size, and have an optimized codepath for this case.  If it's not statically known if this condition is true, it is better to force the unoptimized path unconditionally.  Modify the code so that it no longer compile errors.  In eager mode, you should still use the optimized codepath.  You may find the function `torch.compiler.is_dynamo_compiling()` helpful."]},{"cell_type":"code","metadata":{"originalKey":"1863de38-f87d-4fff-9caf-8c74248eb246","showInput":true,"customInput":null,"language":"python","executionStartTime":1715115273810,"executionStopTime":1715115274684,"serverExecutionDuration":534.74826971069,"collapsed":false,"requestMsgId":"1863de38-f87d-4fff-9caf-8c74248eb246","outputsInitialized":false},"source":["\u0040torch.compile(fullgraph=True, backend=\"eager\")\n","def cf_vb(xst: Tensor, y: Tensor):\n","    xs = xst.tolist()\n","    if all(xs[0] == x for x in xs):\n","        return (y.view(len(xs), -1) + torch.arange(len(xs)).unsqueeze(1)).view(-1)\n","    else:\n","        return torch.concat([t + i for i, t in enumerate(torch.split(y, xs, dim=0))], dim=0)\n","\n","\u0040run_test\n","def test_vb():\n","    assert_eq(cf_vb(torch.tensor([2, 2, 2]), torch.zeros(6, dtype=torch.int64)), torch.tensor([0, 0, 1, 1, 2, 2]))\n","    assert_eq(cf_vb(torch.tensor([2, 3, 1]), torch.zeros(6, dtype=torch.int64)), torch.tensor([0, 0, 1, 1, 1, 2]))"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"originalKey":"7b604611-428d-439c-8212-98b253288fde","showInput":false,"customInput":null,"language":"markdown","outputsInitialized":false},"source":["## Jagged tensors: tensor splits\n","\n","**[tensorsplit]**  Your first idea will be to use `torch._check` to solve the errors. It will not work.  Instead, apply the idea from [changevar] to change the split computation from operating on unbacked SymInts representing offsets, to unbacked SymInts representing sizes.  You may find `torch.diff` and `torch.narrow` useful."]},{"cell_type":"code","metadata":{"originalKey":"ce583d69-b06d-402b-bdac-5b04c12b999f","showInput":true,"customInput":null,"language":"python","executionStartTime":1715115279101,"executionStopTime":1715115280632,"serverExecutionDuration":1243.1421792135,"collapsed":false,"requestMsgId":"ce583d69-b06d-402b-bdac-5b04c12b999f","outputsInitialized":false},"source":["\u0040torch.compile(fullgraph=True, backend=\"eager\")\n","def cf_tensorsplit(x, offsets_t):\n","    offsets = offsets_t.tolist()\n","    rs = []\n","    for start, end in zip(offsets, offsets[1:]):\n","        rs.append(x[start:end])\n","    return rs\n","\n","\u0040run_test\n","def test_tensorsplit():\n","    assert_eq(\n","        cf_tensorsplit(torch.arange(10), torch.tensor([0, 2, 5, 7, 10])),\n","        [torch.tensor([0, 1]), torch.tensor([2, 3, 4]), torch.tensor([5, 6]), torch.tensor([7, 8, 9])]\n","    )"],"execution_count":21,"outputs":[]}]}